import nltk

from nltk.corpus import wordnet

from nltk.tokenize import word_tokenize

from nltk.stem import WordNetLemmatizer

from nltk.sentiment import SentimentIntensityAnalyzer

nltk.download('averaged_perceptron_tagger')

nltk.download('wordnet')

nltk.download('punkt')

nltk.download('vader_lexicon')

def is_ai_generated(text):

    # Check if the text contains patterns commonly found in AI-generated content

    # You can customize these patterns based on your specific needs

    ai_patterns = ["Generated by AI", "Powered by OpenAI", "Artificial Intelligence Essay"]

    for pattern in ai_patterns:

        if pattern.lower() in text.lower():

            return True

    return False

def make_human_like(text):

    # Tokenize the text

    tokens = word_tokenize(text)

    # Perform part-of-speech tagging

    tagged_tokens = nltk.pos_tag(tokens)

    # Lemmatize the tokens to their base form

    lemmatizer = WordNetLemmatizer()

    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag))

                         for token, tag in tagged_tokens]

    # Reconstruct the text from the lemmatized tokens

    human_like_text = ' '.join(lemmatized_tokens)

    return human_like_text

def get_wordnet_pos(treebank_tag):

    # Map POS tags to WordNet tags for lemmatization

    if treebank_tag.startswith('J'):

        return wordnet.ADJ

    elif treebank_tag.startswith('V'):

        return wordnet.VERB

    elif treebank_tag.startswith('N'):

        return wordnet.NOUN

    elif treebank_tag.startswith('R'):

        return wordnet.ADV

    else:

        return wordnet.NOUN

def improve_originality(text):

    # Use sentiment analysis to check the overall sentiment of the text

    sentiment_analyzer = SentimentIntensityAnalyzer()

    sentiment_scores = sentiment_analyzer.polarity_scores(text)

    overall_sentiment = sentiment_scores['compound']

    # If the sentiment is negative, replace some words with their synonyms to improve originality

    if overall_sentiment < 0:

        tokens = word_tokenize(text)

        lemmatizer = WordNetLemmatizer()

        for i in range(len(tokens)):

            token = tokens[i]

            tag = nltk.pos_tag([token])[0][1]

            if tag.startswith('N') or tag.startswith('V') or tag.startswith('J'):

                synsets = wordnet.synsets(token, get_wordnet_pos(tag))

                if synsets:

                    syn_token = synsets[0].lemmas()[0].name()

                    tokens[i] = lemmatizer.lemmatize(syn_token)

        text = ' '.join(tokens)

    return text

# Main function

def process_essay(essay):

    if is_ai_generated(essay):

        human_like_essay = make_human_like(essay)

        essay = improve_originality(human_like_essay)

    return essay

# Example usage

ai_generated_essay = "This essay is generated by AI and contains artificial intelligence-powered content."

human_like_essay = process_essay(ai_generated_essay)

print(human_like_essay)

 

